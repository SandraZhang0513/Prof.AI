import re
import os, json, random, tempfile
import numpy as np
import soundfile as sf
import gradio as gr
import librosa
import whisper
import matplotlib.pyplot as plt
import matplotlib
from matplotlib import font_manager
from pathlib import Path

FONT_PATH = Path("assets/fonts/NotoSansTC-Regular.ttf")

def setup_cjk_font():
    try:
        if FONT_PATH.exists() and FONT_PATH.stat().st_size > 50_000:
            font_manager.fontManager.addfont(str(FONT_PATH))
            fp = font_manager.FontProperties(fname=str(FONT_PATH))
            matplotlib.rcParams["font.family"] = fp.get_name()
        matplotlib.rcParams["axes.unicode_minus"] = False
        return True
    except Exception as e:
        print(f"[Font] load failed: {e}")
        matplotlib.rcParams["axes.unicode_minus"] = False
        return False

CJK_OK = setup_cjk_font()

# =========================
# â­ NEW: æ•™æˆåœ–ç‰‡æ± ï¼ˆæœ¬æ©Ÿç´ æï¼Œå…å³æ™‚ç”Ÿæˆï¼‰
# =========================
INTERVIEWER_DIR = Path("assets/interviewers")

def pick_interviewer():
    """
    å›å‚³ (img_path:str, gender:str) gender in {"male","female"}
    ä½ çš„è³‡æ–™å¤¾éœ€é•·é€™æ¨£ï¼š
      assets/interviewers/male/*.png
      assets/interviewers/female/*.png
    """
    gender = random.choice(["male", "female"])
    folder = INTERVIEWER_DIR / gender
    imgs = []
    if folder.exists():
        imgs = list(folder.glob("*.png")) + list(folder.glob("*.jpg")) + list(folder.glob("*.jpeg"))
    if not imgs:
        # æ‰¾ä¸åˆ°å°±é€€å› logoï¼Œé¿å…æ•´å€‹ç¨‹å¼ç‚¸æ‰
        return "assets/logo.png", "female"
    img_path = random.choice(imgs)
    return str(img_path), gender


# =========================
# 0) æƒ…å¢ƒè¨­å®šï¼ˆé¡Œç›®æ±  + æ¬Šé‡ï¼‰
# =========================
SCENES = {
    "university": {
        "label": "å¤§å­¸ç³»æ‰€ç”³è«‹é¢è©¦",
        "topics": {"è‡ªæˆ‘ä»‹ç´¹", "å­¸ç¿’è¨ˆç•«", "å­¸ç¿’å‹•æ©Ÿ", "æœªä¾†è¦åŠƒ", "ç¤¾æœƒé—œæ‡·"},
        "weights": {"coverage": 0.35, "structure": 0.25, "semantic": 0.20, "fluency": 0.10, "pitch": 0.10},
    },
    "graduate": {
        "label": "ç ”ç©¶æ‰€å£è©¦ / æ¨ç”„",
        "topics": {"é–±è®€èˆ‡ç ”ç©¶", "é“å¾·èˆ‡è²¬ä»»", "è·¨åŸŸå­¸ç¿’", "å•é¡Œè§£æ±º", "å°ˆé¡Œç¶“é©—"},
        "weights": {"coverage": 0.25, "structure": 0.30, "semantic": 0.30, "fluency": 0.10, "pitch": 0.05},
    },
    "hr": {
        "label": "ä¼æ¥­ HR åˆéšé¢è©¦",
        "topics": {"åœ˜éšŠåˆä½œ", "æºé€šè¡¨é”", "æ™‚é–“ç®¡ç†", "å¤±æ•—ç¶“é©—", "è‡¨å ´è¡¨é”", "å•é¡Œè§£æ±º"},
        "weights": {"coverage": 0.20, "structure": 0.20, "semantic": 0.15, "fluency": 0.30, "pitch": 0.15},
    },
}
SCENE_CHOICES = [(v["label"], k) for k, v in SCENES.items()]

# =========================
# 1) é¡Œåº«ï¼šè®€å–èˆ‡æŠ½é¡Œ
# =========================
def load_questions(path="questions/professor.json"):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data["questions"]

def filter_by_scene(questions, scene_key: str):
    scene = SCENES.get(scene_key, SCENES["university"])
    topics = scene["topics"]
    out = []
    for q in questions:
        if q.get("id") == "p-001":
            out.append(q)
        elif q.get("topic") in topics:
            out.append(q)
    return out

def pick_five_with_intro(questions, intro_id="p-001"):
    intro_list = [q for q in questions if q.get("id") == intro_id]
    if not intro_list:
        raise ValueError(f"Intro question '{intro_id}' not found.")
    intro_q = intro_list[0]

    pool = [q for q in questions if q.get("id") != intro_id]
    if len(pool) < 4:
        raise ValueError("Not enough questions for this scene (need >= 4 excluding intro).")

    others = random.sample(pool, k=4)
    return [intro_q] + others

# =========================
# 2) ASRï¼šWhisper
# =========================
_whisper = None
def load_asr():
    global _whisper
    if _whisper is None:
        size = os.environ.get("WHISPER_SIZE", "small")  # tiny/base/small/medium
        _whisper = whisper.load_model(size)
    return _whisper

def transcribe_zh(audio_path, initial_prompt=None):
    m = load_asr()
    res = m.transcribe(
        audio_path,
        language="zh",
        task="transcribe",
        initial_prompt=initial_prompt or ""
    )
    return (res.get("text") or "").strip()

# =========================
# 3) å…§å®¹åˆ†æï¼šæŠŠ key_points è½‰æˆã€Œå¯å‘½ä¸­çš„é—œéµè©ã€
# =========================
TRANSITIONS = ["é¦–å…ˆ", "ç¬¬ä¸€", "æ¥è‘—", "ç„¶å¾Œ", "å› æ­¤", "æ‰€ä»¥", "æœ€å¾Œ", "ç¸½çµ", "æ­¤å¤–", "å¦å¤–"]
SEMANTIC_KEYWORDS = [
    "å› ç‚º","æ‰€ä»¥","ä¾‹å¦‚","æ¯”å¦‚","èˆ‰ä¾‹","æˆ‘è¦ºå¾—","æˆ‘å­¸åˆ°","æˆ‘ç™¼ç¾",
    "ç¶“é©—","æŒ‘æˆ°","æˆæœ","å­¸ç¿’","åæ€","å›°é›£","æ”¹å–„","åƒ¹å€¼","æ”¶ç©«","åŠªåŠ›"
]

def _contains_any(text: str, words):
    return any(w in text for w in words)

KEYPOINT_HINTS = {
    "å€‹äººèƒŒæ™¯æ¸…æ¥š": ["æˆ‘å«", "æˆ‘ä¾†è‡ª", "å°±è®€", "é«˜ä¸­", "ç§‘ç³»", "èƒŒæ™¯", "ç¶“æ­·", "ç¤¾åœ˜", "å¿—å·¥", "å°ˆé¡Œ"],
    "ç”³è«‹å‹•æ©Ÿæ˜ç¢º": ["å› ç‚º", "æ‰€ä»¥", "æƒ³è¦", "å¸Œæœ›", "å‹•æ©Ÿ", "åŸå› ", "èˆˆè¶£", "ç†±å¿±"],
    "èˆ‡ç§‘ç³»é€£çµå…·é«”": ["æœ¬ç³»", "ç§‘ç³»", "ç³»ä¸Š", "èª²ç¨‹", "å­¸ç¨‹", "é ˜åŸŸ", "æ–¹å‘", "é©åˆ", "ç›¸é—œ", "å»åˆ"],

    "å­¸ç¿’æ–¹å‘æ˜ç¢º": ["æˆ‘æœƒ", "æˆ‘æƒ³", "ç›®æ¨™", "æ–¹å‘", "è¦åŠƒ", "è¨ˆç•«"],
    "å…·é«”èª²ç¨‹æˆ–ä¸»é¡Œä¾‹å­": ["èª²ç¨‹", "å°ˆé¡Œ", "ä¸»é¡Œ", "ç ”ç©¶", "é ˜åŸŸ", "ä¾‹å¦‚", "æ¯”å¦‚"],
    "å±•ç¾è‡ªä¸»è¦åŠƒèƒ½åŠ›": ["å®‰æ’", "æ™‚é–“", "è¦åŠƒ", "è¡Œäº‹æ›†", "å¾…è¾¦", "ç›®æ¨™", "æ­¥é©Ÿ"],

    "éç¨‹èˆ‡æˆæœå…·é«”": ["éç¨‹", "çµæœ", "æˆæœ", "å®Œæˆ", "æå‡", "æˆæ•ˆ", "å­¸åˆ°"],
    "å€‹äººè²¢ç»æ¸…æ¥š": ["æˆ‘è² è²¬", "æˆ‘ä¸»è¦", "æˆ‘çš„è§’è‰²", "åˆ†å·¥", "è²¢ç»"],
    "å•é¡Œè§£æ±ºèƒ½åŠ›": ["è§£æ±º", "è™•ç†", "æ”¹å–„", "å˜—è©¦", "æ–¹æ³•", "ç­–ç•¥"],

    "å‹•æ©ŸçœŸèª å…·é«”": ["å› ç‚º", "å¥‘æ©Ÿ", "é–‹å§‹", "èˆˆè¶£", "å–œæ­¡", "æƒ³æ·±å…¥"],
    "æœ‰ä¾‹å­æ”¯æŒ": ["ä¾‹å¦‚", "æ¯”å¦‚", "èˆ‰ä¾‹", "åƒåŠ ", "çœ‹é", "åšé", "ç¶“é©—"],
    "èˆ‡æœªä¾†è¦åŠƒé€£çµ": ["æœªä¾†", "ç•¢æ¥­", "ç›®æ¨™", "æƒ³æˆç‚º", "è¦åŠƒ", "æ–¹å‘"],

    "çŸ­ä¸­é•·æœŸç›®æ¨™": ["çŸ­æœŸ", "ä¸­æœŸ", "é•·æœŸ", "æœªä¾†", "ç›®æ¨™"],
    "å…·é«”æ­¥é©Ÿ": ["æ­¥é©Ÿ", "è¨ˆç•«", "æ–¹æ³•", "å®‰æ’", "æº–å‚™"],
    "å½ˆæ€§èˆ‡åæ€": ["å¦‚æœ", "èª¿æ•´", "åæ€", "æª¢è¨", "æ”¹é€²"],

    "åˆä½œæƒ…å¢ƒå…·é«”": ["ä¸€èµ·", "åœ˜éšŠ", "åˆä½œ", "åŒå­¸", "å°çµ„"],
    "æºé€šå”èª¿": ["æºé€š", "å”èª¿", "è¨è«–", "å…±è­˜", "åˆ†å·¥"],
    "åæ€èˆ‡æˆé•·": ["å­¸åˆ°", "æ”¶ç©«", "åæ€", "ä¸‹æ¬¡", "æ”¹é€²"],

    "å•é¡Œæè¿°æ¸…æ¥š": ["å•é¡Œ", "å›°é›£", "æŒ‘æˆ°", "å¡é—œ"],
    "è¡Œå‹•æœ‰æ¢ç†": ["é¦–å…ˆ", "æ¥è‘—", "ç„¶å¾Œ", "æœ€å¾Œ", "æ­¥é©Ÿ", "å®‰æ’"],
    "åæ€å…·æ·±åº¦": ["åæ€", "å­¸åˆ°", "æ”¶ç©«", "åƒ¹å€¼", "ä¸‹æ¬¡"],

    "æ€è€ƒé‚è¼¯æ¸…æ¥š": ["æˆ‘æœƒå…ˆ", "æˆ‘å…ˆæƒ³", "æ•´ç†", "è¦é»", "é¦–å…ˆ"],
    "å†·éœçµ„ç¹”å›ç­”": ["å…ˆ", "æ•´ç†", "æƒ³ä¸€ä¸‹", "é‡é»", "å†å›ç­”"],
    "èˆ‰ä¾‹èªªæ˜": ["ä¾‹å¦‚", "æ¯”å¦‚", "èˆ‰ä¾‹", "æ›¾ç¶“"],

    "å…§å®¹æŒæ¡": ["å…§å®¹", "é‡é»", "ä¸»æ—¨", "ä½œè€…", "è§€é»"],
    "å€‹äººè¦‹è§£": ["æˆ‘èªç‚º", "æˆ‘è¦ºå¾—", "æˆ‘çœ‹æ³•", "å•Ÿç™¼", "åæ€"],
    "èˆ‡å­¸ç³»é€£çµ": ["æœ¬ç³»", "ç§‘ç³»", "èª²ç¨‹", "é ˜åŸŸ", "ç›¸é—œ"],

    "å…·é«”æ–¹æ³•": ["è¡Œäº‹æ›†", "å¾…è¾¦", "æ¸…å–®", "è¦åŠƒ", "å®‰æ’"],
    "å„ªå…ˆç´šèˆ‡è¦åŠƒ": ["å„ªå…ˆ", "é‡è¦", "ç·Šæ€¥", "æ’åº", "å®‰æ’"],
    "è‡ªæˆ‘æª¢æ ¸": ["æª¢æ ¸", "å›é¡§", "ç¢ºèª", "èª¿æ•´"],

    "è·¨åŸŸé€£çµ": ["çµåˆ", "æ•´åˆ", "è·¨åŸŸ", "ä¸åŒé ˜åŸŸ", "é€£çµ"],
    "å‰µæ„æ€è€ƒ": ["æƒ³åˆ°", "å˜—è©¦", "å‰µæ–°", "ç™¼æƒ³"],
    "å¯¦ä½œæˆæœ": ["æˆæœ", "å®Œæˆ", "åšå‡º", "æˆæ•ˆ"],

    "èª å¯¦æè¿°": ["æˆ‘å¤±æ•—", "ä¸å¦‚é æœŸ", "æ²’æœ‰åšå¥½", "ç•¶æ™‚"],
    "å…·é«”åæ€": ["åæ€", "å­¸åˆ°", "æ”¶ç©«", "åŸå› "],
    "æ”¹é€²ç­–ç•¥": ["æ”¹é€²", "èª¿æ•´", "ä¸‹æ¬¡", "æ–¹æ³•"],

    "æ¸…æ¥šè¡¨é”": ["æˆ‘æœƒå…ˆèªª", "é‡é»", "æ•´ç†", "æ¸…æ¥š"],
    "å‚¾è½èˆ‡åŒç†": ["å‚¾è½", "ç†è§£", "åŒç†", "å°Šé‡"],
    "å…±è­˜ç­–ç•¥": ["å…±è­˜", "æŠ˜è¡·", "å”èª¿", "è¨è«–"],

    "è­°é¡ŒèªçŸ¥": ["æˆ‘é—œå¿ƒ", "è­°é¡Œ", "ç¾è±¡", "å•é¡Œ"],
    "è¡Œå‹•æˆ–è§€å¯Ÿ": ["è§€å¯Ÿ", "åƒèˆ‡", "è¡Œå‹•", "ç¶“é©—"],
    "èˆ‡å­¸ç³»é€£çµèˆ‡å±•æœ›": ["æœ¬ç³»", "ç›¸é—œ", "æœªä¾†", "æŠ•å…¥"],

    "å€«ç†åŸå‰‡": ["èª ä¿¡", "å€«ç†", "åŸå‰‡", "å°Šé‡", "å…¬å¹³"],
    "å…¼é¡§å…¬å¹³èˆ‡æ•ˆç‡": ["å…¬å¹³", "æ•ˆç‡", "é€æ˜", "è²¬ä»»", "åˆ†å·¥"],
}

def analyze_content(transcript: str, question: dict):
    text = (transcript or "").strip()
    kps = question.get("key_points", []) or []

    if not kps:
        return {
            "coverage_pct": 0, "hits": [], "misses": [],
            "structure_score": 0.0, "semantic_score": 0.0,
            "content_score_raw": 0, "content_advice": "æœ¬é¡Œç„¡è¨­å®š key pointsã€‚"
        }

    hits, misses = [], []
    for kp in kps:
        hints = KEYPOINT_HINTS.get(kp)
        if hints:
            hit = _contains_any(text, hints)
        else:
            hit = any(token in text for token in re.split(r"[ã€ï¼Œ,ã€‚\s]+", kp) if token)
        (hits if hit else misses).append(kp)

    coverage = len(hits) / len(kps)
    structure_score = 1.0 if _contains_any(text, TRANSITIONS) else 0.0

    sentence_count = len([s for s in re.split(r"[ã€‚ï¼ï¼Ÿ!?.]", text) if s.strip()])
    semantic_hits = sum(1 for w in SEMANTIC_KEYWORDS if w in text)
    semantic_score = min(1.0, (sentence_count / 3) * 0.4 + (semantic_hits / 5) * 0.6)

    content_score_raw = int(round(
        coverage * 100 * 0.5 +
        structure_score * 100 * 0.2 +
        semantic_score * 100 * 0.3
    ))

    adv = []
    if coverage < 0.7 and misses:
        adv.append(f"å¯å†è£œå……ã€Œ{'ã€'.join(misses[:3])}ã€ç­‰é‡é»ã€‚")
    if structure_score < 1.0:
        adv.append("å¯åŠ å…¥è½‰æŠ˜è©ï¼ˆå¦‚ã€é¦–å…ˆã€æ¥è‘—ã€å› æ­¤ã€ï¼‰æå‡æ¢ç†æ€§ã€‚")
    if semantic_score < 0.6:
        adv.append("å»ºè­°è£œå……å…·é«”ä¾‹å­æˆ–åæ€å¥ï¼Œè®“å…§å®¹æ›´æœ‰èªªæœåŠ›ã€‚")
    if not adv:
        adv.append("å…§å®¹å®Œæ•´ã€æ¢ç†æ¸…æ¥šï¼Œå…·è‰¯å¥½è«–è¿°æ·±åº¦ã€‚")

    return {
        "coverage_pct": int(round(coverage * 100)),
        "hits": hits,
        "misses": misses,
        "structure_score": float(structure_score),
        "semantic_score": float(round(semantic_score, 2)),
        "content_score_raw": int(content_score_raw),
        "content_advice": " ".join(adv)
    }

# =========================
# 4) èªéŸ³ç‰¹å¾µï¼šèªé€Ÿ/åœé “/éŸ³é«˜è®ŠåŒ– => åˆ†æ•¸
# =========================
def clamp(x, lo=0, hi=100):
    return max(lo, min(hi, int(round(x))))

def analyze_audio_and_text(audio_np, sr, zh_text):
    duration = max(1e-6, len(audio_np) / float(sr))
    char_per_min = (len(zh_text) / duration) * 60.0

    if len(audio_np) < 1024:
        metrics = {
            "transcript": zh_text, "duration_sec": round(duration, 2),
            "chars_per_min": round(char_per_min, 1),
            "pauses_(>0.3s)_count": 0, "pitch_variation_CV": 0.0,
        }
        return metrics, ["éŒ„éŸ³å¤ªçŸ­ï¼Œè«‹éŒ„è‡³å°‘ 10 ç§’å†åˆ†æã€‚"]

    frames = librosa.util.frame(audio_np, frame_length=1024, hop_length=512)
    energy = (frames**2).mean(axis=0)
    thr = energy.mean() * 0.3
    voiced = energy > thr

    pauses = 0
    i = 0
    hop_dur = 512 / float(sr)
    while i < len(voiced):
        if not voiced[i]:
            start = i
            while i < len(voiced) and not voiced[i]:
                i += 1
            dur = (i - start) * hop_dur
            if dur >= 0.3:
                pauses += 1
        i += 1

    try:
        f0 = librosa.yin(audio_np, fmin=50, fmax=400, sr=sr, frame_length=2048)
        f0 = f0[np.isfinite(f0)]
        pitch_var = float(np.std(f0) / np.mean(f0)) if len(f0) > 0 else 0.0
    except Exception:
        pitch_var = 0.0

    advice = []
    if char_per_min > 180:
        advice.append("èªé€Ÿåå¿«ï¼šé—œéµå¥å‰å…ˆåœ 0.3â€“0.5 ç§’ï¼Œè®“é‡é»æ›´æ¸…æ¥šã€‚")
    elif char_per_min < 100:
        advice.append("èªé€Ÿåæ…¢ï¼šå¯é©åº¦åŠ å¿«ï¼Œé¿å…éé•·åœé “ã€‚")
    if pauses >= 6:
        advice.append("åœé “è¼ƒå¤šï¼šå…ˆåˆ—å‡º 2â€“3 å€‹è¦é»å†å›ç­”ï¼Œé™ä½å¡é “ã€‚")
    if pitch_var < 0.10:
        advice.append("èªèª¿èµ·ä¼è¼ƒå°‘ï¼šå¯åœ¨é‡é»å¥æ”¾æ…¢ã€åŠ å¼·æŠ‘æšã€‚")
    if not advice:
        advice.append("æ•´é«”è¡¨é”ç©©å®šï¼šç¶­æŒç¯€å¥èˆ‡èªæ°£ï¼Œä¸‹ä¸€æ­¥å¯åŠ æ·±å…§å®¹ã€‚")

    metrics = {
        "transcript": zh_text,
        "duration_sec": round(duration, 2),
        "chars_per_min": round(char_per_min, 1),
        "pauses_(>0.3s)_count": int(pauses),
        "pitch_variation_CV": round(pitch_var, 3),
    }
    return metrics, advice

def compute_fluency_and_pitch_scores(metrics: dict):
    cpm = float(metrics.get("chars_per_min", 0))
    pauses = int(metrics.get("pauses_(>0.3s)_count", 0))
    pitch = float(metrics.get("pitch_variation_CV", 0))

    if 120 <= cpm <= 160:
        speed_score = 100
    elif 100 <= cpm < 120 or 160 < cpm <= 180:
        speed_score = 75
    else:
        speed_score = 50

    if pauses <= 2:
        pause_score = 100
    elif pauses <= 5:
        pause_score = 75
    else:
        pause_score = 50

    fluency = clamp(speed_score * 0.4 + pause_score * 0.6)

    if pitch >= 0.15:
        pitch_score = 100
    elif pitch >= 0.10:
        pitch_score = 75
    else:
        pitch_score = 50

    return fluency, pitch_score, speed_score, pause_score

def get_grade(final_score: int) -> str:
    if final_score >= 90: return "A"
    if final_score >= 80: return "B"
    if final_score >= 70: return "C"
    if final_score >= 60: return "D"
    return "E"

# =========================
# 5) é›·é”åœ–
# =========================
def render_radar_fig(radar: dict, title="Radar (0-100)"):
    labels = ["è¦†è“‹", "æ¢ç†", "æ·±åº¦", "æµæš¢", "æŠ‘æš"]
    keys = ["coverage", "structure", "semantic", "fluency", "pitch"]
    values = [float(radar.get(k, 0)) for k in keys]
    values += values[:1]

    N = len(labels)
    angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()
    angles += angles[:1]

    fig = plt.figure(figsize=(4.2, 4.2), dpi=120)
    ax = plt.subplot(111, polar=True)
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_thetagrids(np.degrees(angles[:-1]), labels, fontsize=10)
    ax.set_ylim(0, 100)
    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)
    ax.set_title(title, pad=18, fontsize=11)
    ax.grid(True, alpha=0.3)
    return fig

# =========================
# â­ NEWï¼šå³ä¸Šè§’åˆ†æ•¸å¾½ç« ç”¨çš„ HTML
# =========================
def score_badge_html(score, grade=None):
    s = "--" if score is None else str(int(score))
    g = "" if not grade else f"<div class='score-grade'>Grade {grade}</div>"
    return f"""
    <div class="score-badge">
        <div class="score-num">{s}</div>
        {g}
    </div>
    """

# =========================
# 6) Gradio å›å‘¼
# =========================
HINT_VOCAB = "é¢è©¦ è‡ªæˆ‘ä»‹ç´¹ æ•™æˆ å­¸ç¿’è¨ˆç•« åœ‹ç«‹å°ä¸­æ•™è‚²å¤§å­¸ å…§å®¹ç§‘æŠ€å­¸ç³» æ•™è‚²å­¸ç¨‹ å°ˆé¡Œå ±å‘Š åœ˜éšŠåˆä½œ"

def start_session(scene_key):
    all_qs = load_questions()
    scene_qs = filter_by_scene(all_qs, scene_key)
    selected = pick_five_with_intro(scene_qs, intro_id="p-001")

    idx = 0
    q = selected[idx]
    q_text = f"ç¬¬1é¡Œï¼š{q['prompt']}"

    # â­ NEWï¼šSTART å°±æŠ½ä¸€å¼µæ•™æˆåœ–ï¼‹æ€§åˆ¥
    interviewer_img, interviewer_gender = pick_interviewer()

    # â­ NEWï¼šSTART æ™‚å³ä¸Šè§’å…ˆé¡¯ç¤º --
    badge = score_badge_html(None, None)

    return selected, idx, q_text, "", None, None, None, interviewer_img, interviewer_gender, badge

def analyze_and_next(audio, selected, idx, scene_key):
    if audio is None:
        # â­ NEWï¼šæ²’æœ‰éŒ„éŸ³æ™‚ï¼Œå¾½ç« ä¸è®Šï¼ˆç”¨ gr.update()ï¼‰
        return gr.JSON.update(value=None), None, "è«‹å…ˆéŒ„éŸ³å†é€å‡ºã€‚", gr.update(), idx, selected, gr.update(), gr.update(), gr.update()

    if isinstance(audio, tuple):
        sr, y = audio
    else:
        y, sr = librosa.load(audio, sr=16000, mono=True)

    current_q = selected[idx]
    question_prompt = current_q.get("prompt", "")

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        sf.write(tmp.name, y, sr)
        text = transcribe_zh(tmp.name, initial_prompt=HINT_VOCAB)

    metrics, prosody_adv_list = analyze_audio_and_text(y, sr, text)
    content_res = analyze_content(text, current_q)

    radar = {
        "coverage": clamp(content_res["coverage_pct"]),
        "structure": clamp(content_res["structure_score"] * 100),
        "semantic": clamp(content_res["semantic_score"] * 100),
    }

    fluency, pitch_score, speed_score, pause_score = compute_fluency_and_pitch_scores(metrics)
    radar["fluency"] = fluency
    radar["pitch"] = pitch_score

    scene = SCENES.get(scene_key, SCENES["university"])
    w = scene["weights"]

    final_score = int(round(
        radar["coverage"] * w["coverage"] +
        radar["structure"] * w["structure"] +
        radar["semantic"] * w["semantic"] +
        radar["fluency"] * w["fluency"] +
        radar["pitch"] * w["pitch"]
    ))
    grade = get_grade(final_score)

    metrics_out = {
        "scene": scene["label"],
        "question": question_prompt,
        "transcript": metrics.get("transcript", ""),
        "duration_sec": metrics.get("duration_sec"),
        "chars_per_min": metrics.get("chars_per_min"),
        "pauses_(>0.3s)_count": metrics.get("pauses_(>0.3s)_count"),
        "pitch_variation_CV": metrics.get("pitch_variation_CV"),
        "content_hits": content_res["hits"],
        "content_misses": content_res["misses"],
        "radar(0-100)": radar,
        "weights": w,
        "final_score(0-100)": final_score,
        "grade": grade,
        "debug_speed_score": speed_score,
        "debug_pause_score": pause_score,
        "debug_pitch_score": pitch_score,
    }

    summary = (
        f"ã€æƒ…å¢ƒã€‘{scene['label']}\n"
        f"ã€æœ¬é¡Œã€‘{question_prompt}\n"
        f"ã€ç¸½åˆ†ã€‘{final_score} / 100ï¼ˆç­‰ç´š {grade}ï¼‰\n"
        f"ã€é›·é”ã€‘è¦†è“‹{radar['coverage']}ã€æ¢ç†{radar['structure']}ã€æ·±åº¦{radar['semantic']}ã€æµæš¢{radar['fluency']}ã€æŠ‘æš{radar['pitch']}\n"
    )

    advice = (
        summary
        + "\nã€è¡¨é”å»ºè­°ã€‘\n" + "\n".join(prosody_adv_list)
        + "\n\nã€å…§å®¹å»ºè­°ã€‘\n" + content_res["content_advice"]
    )

    radar_fig = render_radar_fig(radar, title=f"{scene['label']} Radar (0-100)")

    idx_next = idx + 1
    if idx_next < len(selected):
        q_next = selected[idx_next]
        q_text = f"ç¬¬{idx_next+1}é¡Œï¼š{q_next['prompt']}"
    else:
        q_text = "âœ… å…¨éƒ¨ 5 é¡Œå®Œæˆï¼å¯ä»¥é‡æ–°é–‹å§‹ã€‚"

    # â­ NEWï¼šæ¯é¡Œåˆ†æå®Œå°±æ›ä¸€å¼µæ•™æˆåœ–ï¼‹æ€§åˆ¥
    interviewer_img, interviewer_gender = pick_interviewer()

    # â­ NEWï¼šæ›´æ–°å³ä¸Šè§’å¾½ç« 
    badge = score_badge_html(final_score, grade)

    return metrics_out, radar_fig, advice, q_text, idx_next, selected, interviewer_img, interviewer_gender, badge

# =========================
# 7) UI
# =========================
CSS = """
.gradio-container { max-width: 100% !important; }

/* â­ NEWï¼šå³ä¸Šè§’ç¸½åˆ†å¾½ç« ï¼ˆå›ºå®šåœ¨ç•«é¢å³ä¸Šï¼‰ */
.score-badge {
  position: fixed;
  top: 18px;
  right: 18px;
  width: 92px;
  height: 92px;
  border-radius: 18px;
  border: 4px solid #7c5cff;
  background: #ffffff;
  box-shadow: 0 10px 25px rgba(0,0,0,0.10);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  z-index: 9999;
}
.score-num {
  font-size: 44px;
  font-weight: 800;
  color: #7c5cff;
  line-height: 1.0;
}
.score-grade {
  margin-top: 4px;
  font-size: 12px;
  font-weight: 700;
  color: #7c5cff;
}
"""

with gr.Blocks(title="AI Mock Interview (Professor)", css=CSS) as demo:
    gr.Markdown("## ğŸ‘©â€ğŸ« AI é¢è©¦ç·´ç¿’\nå›ºå®šç¬¬ 1 é¡Œè‡ªæˆ‘ä»‹ç´¹ï¼Œå…¶é¤˜éš¨æ©Ÿ 4 é¡Œï¼Œå…± 5 é¡Œã€‚")

    # â­ NEWï¼šç”¨ State å­˜æ€§åˆ¥ï¼Œè®“æœ—è®€è²éŸ³å¯ä»¥å°ä¸Šåœ–ç‰‡
    interviewer_gender_state = gr.State("female")

    # â­ NEWï¼šå³ä¸Šè§’ç¸½åˆ†å¾½ç« ï¼ˆç”¨ HTML å‘ˆç¾ï¼‰
    score_box = gr.HTML(value=score_badge_html(None, None))

    with gr.Row(equal_height=True):
        
        # âœ… å·¦é‚Šï¼šæ•™æˆåœ–ï¼ˆ2/3ï¼‰
        with gr.Column(scale=2):
            interviewer_img = gr.Image(value="assets/logo.png", show_label=False, interactive=False, height=360)

        # âœ… å³é‚Šï¼šæ“ä½œå€ï¼ˆ1/3ï¼‰
        with gr.Column():
            scene_dd = gr.Dropdown(choices=SCENE_CHOICES, value="university", label="é¸æ“‡é¢è©¦æƒ…å¢ƒï¼ˆå½±éŸ¿é¡Œç›®æ± ï¼‹è©•åˆ†æ¬Šé‡ï¼‰")
            start_btn = gr.Button("START", variant="primary")
            speak_btn = gr.Button("ğŸ”Š", scale=1)
            question_box = gr.Textbox(label="é¡Œç›®", interactive=False, lines=3)

    # ä½ åŸæœ¬çš„ speechSynthesis ä¿ç•™ï¼ˆç…§ä½ çš„è¦å‰‡ï¼šä¸å‹•åŸæœ¬çµæ§‹ï¼‰
    speak_btn.click(
        fn=None,
        inputs=[question_box, interviewer_gender_state],
        outputs=None,
        js=r"""
        (text, gender)=>{
            window.speechSynthesis.cancel();
            const u = new SpeechSynthesisUtterance(text);
            u.lang = "zh-TW";
            u.rate = 1.0;

            const pickVoice = () => {
                const voices = speechSynthesis.getVoices() || [];
                if (!voices.length) return;

                const zhVoices = voices.filter(v => (v.lang || "").toLowerCase().includes("zh"));
                const pool = zhVoices.length ? zhVoices : voices;

                const isFemale = (v) => /female|woman|mei|ting|xiaomei|hui/i.test((v.name||"") + " " + (v.voiceURI||""));
                const isMale   = (v) => /male|man|wei|jun|xiaojun|kang/i.test((v.name||"") + " " + (v.voiceURI||""));

                let chosen = null;
                if (gender === "female") chosen = pool.find(isFemale) || pool[0];
                else chosen = pool.find(isMale) || pool[0];

                if (chosen) u.voice = chosen;
            };

            pickVoice();
            setTimeout(pickVoice, 200);

            speechSynthesis.speak(u);
        }
        """
    )

    audio_in = gr.Audio(sources=["microphone"], type="numpy", label="éŒ„éŸ³ï¼ˆå»ºè­° 60â€“90 ç§’ï¼‰")
    submit_btn = gr.Button("ğŸ§ª åˆ†æä¸¦é€²åˆ°ä¸‹ä¸€é¡Œ", variant="secondary")

    with gr.Row():
        metrics_out = gr.JSON(label="åˆ†ææŒ‡æ¨™ï¼ˆå« transcript / å‘½ä¸­/æœªå‘½ä¸­ / æ¬Šé‡ / åˆ†æ•¸ï¼‰")
        radar_out = gr.Plot(label="é›·é”åœ–ï¼ˆ0-100ï¼‰")
        advice_out = gr.Textbox(label="å»ºè­°ï¼ˆå«ç¸½åˆ†/é›·é”/å…§å®¹/è¡¨é”ï¼‰", lines=12)

    # â­ NEWï¼šå³ä¸‹è§’ç­‰ç´šèªªæ˜ï¼ˆå¯æ”¶åˆã€ä¸å½±éŸ¿ UIï¼›ä¸æƒ³è¦å¯æ•´æ®µåˆªï¼‰
    with gr.Accordion("ç­‰ç´šèªªæ˜ï¼ˆå¯æ”¶åˆï¼‰", open=False):
        gr.Markdown(
            """
| åˆ†æ•¸ | ç­‰ç´š | èªªæ˜ |
|---:|:---:|:---|
| 90â€“100 | A | è¡¨ç¾å„ªç§€ï¼šå…§å®¹å®Œæ•´ã€æ¢ç†æ¸…æ™°ã€è¡¨é”è‡ªç„¶ |
| 80â€“89 | B | è¡¨ç¾è‰¯å¥½ï¼šé‡é»å¤§è‡´åˆ°ä½ï¼Œå¯å†å¼·åŒ–ä¾‹å­/åæ€ |
| 70â€“79 | C | åŸºæœ¬å¯è¡Œï¼šå…§å®¹æˆ–çµæ§‹å°šå¯ï¼Œä½†èªªæœåŠ›ä¸è¶³ |
| 60â€“69 | D | éœ€åŠ å¼·ï¼šé‡é»ç¼ºæ¼æˆ–è¡¨é”ä¸ç©©ã€åœé “è¼ƒå¤š |
| < 60 | E | å»ºè­°é‡ç·´ï¼šå…ˆæº–å‚™æ¡†æ¶èˆ‡é—œéµå¥ï¼Œå†é‡æ–°éŒ„éŸ³ |
            """
        )

    selected_state = gr.State([])
    idx_state = gr.State(0)

    # â­ NEWï¼šSTART outputs å¤šæ¥ã€Œé¢è©¦å®˜åœ–ã€é¢è©¦å®˜æ€§åˆ¥ã€å³ä¸Šè§’å¾½ç« ã€
    start_btn.click(
        fn=start_session,
        inputs=[scene_dd],
        outputs=[selected_state, idx_state, question_box, advice_out, metrics_out, audio_in, radar_out, interviewer_img, interviewer_gender_state, score_box]
    )

    # â­ NEWï¼šSUBMIT outputs å¤šæ¥ã€Œé¢è©¦å®˜åœ–ã€é¢è©¦å®˜æ€§åˆ¥ã€å³ä¸Šè§’å¾½ç« ã€ï¼ˆæ¯é¡Œéƒ½æ›ï¼‰
    submit_btn.click(
        fn=analyze_and_next,
        inputs=[audio_in, selected_state, idx_state, scene_dd],
        outputs=[metrics_out, radar_out, advice_out, question_box, idx_state, selected_state, interviewer_img, interviewer_gender_state, score_box]
    )

if __name__ == "__main__":
    demo.launch()